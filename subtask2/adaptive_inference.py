# æ”¹è¿›çš„æ¨ç†è„šæœ¬ - ç¯å¢ƒè‡ªé€‚åº”ç‰ˆæœ¬

import cv2
import torch
import numpy as np
import json
from collections import Counter
from model import GestureGRU
from sklearn.preprocessing import StandardScaler
from feature_extractor import FeatureExtractor

def adaptive_inference():
    """è‡ªé€‚åº”æ¨ç†ï¼Œæ ¹æ®ç¯å¢ƒè°ƒæ•´å‚æ•°"""

    print("ğŸ¯ å¯åŠ¨ç¯å¢ƒè‡ªé€‚åº”æ‰‹åŠ¿è¯†åˆ«...")

    # åˆå§‹åŒ–ç»„ä»¶
    try:
        extractor = FeatureExtractor()
        print("âœ… ç‰¹å¾æå–å™¨åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç‰¹å¾æå–å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # åŠ è½½æ¨¡å‹
    try:
        model = GestureGRU()
        model.load_state_dict(torch.load('gesture_gru_cv.pth'))
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    # åŠ è½½æ ‡å‡†åŒ–å™¨
    try:
        train_data = np.load('hand_gesture_data.npz')
        X_train = train_data['X']
        original_shape = X_train.shape
        X_reshaped = X_train.reshape(-1, original_shape[-1])
        scaler = StandardScaler()
        scaler.fit(X_reshaped)
        print("âœ… æ ‡å‡†åŒ–å™¨åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ ‡å‡†åŒ–å™¨åŠ è½½å¤±è´¥: {e}")
        return

    # åŠ è½½ç±»åˆ«
    try:
        with open('classes.json', 'r') as f:
            GESTURES = json.load(f)
            GESTURES = {int(k): v for k, v in GESTURES.items()}
        print("âœ… ç±»åˆ«åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âŒ ç±»åˆ«åŠ è½½å¤±è´¥: {e}")
        return

    # è‡ªé€‚åº”å‚æ•°
    cap = cv2.VideoCapture(1)
    if not cap.isOpened():
        print("âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´")
        return

    # ç¯å¢ƒåˆ†æé˜¶æ®µ
    print("\nğŸ“Š ç¯å¢ƒåˆ†æä¸­...")
    env_stats = analyze_environment(cap, extractor)
    adaptive_params = get_adaptive_parameters(env_stats)

    print("\nğŸ›ï¸  è‡ªé€‚åº”å‚æ•°:")
    print(f"   ç½®ä¿¡åº¦é˜ˆå€¼: {adaptive_params['confidence_threshold']}")
    print(f"   ç¼“å†²åŒºå¤§å°: {adaptive_params['buffer_size']}")
    print(f"   æ£€æµ‹è´¨é‡é˜ˆå€¼: {adaptive_params['quality_threshold']}")

    # æ¨ç†é˜¶æ®µ
    sequence = []
    seq_len = 30
    predictions = []
    confidences = []

    print("\nğŸš€ å¼€å§‹æ¨ç† (æŒ‰'q'é€€å‡º)...")

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # æå–ç‰¹å¾
        try:
            keypoints, pose_results, hand_results = extractor.extract_features(frame)
        except Exception as e:
            print(f"ç‰¹å¾æå–é”™è¯¯: {e}")
            continue

        # ç»˜åˆ¶æ£€æµ‹ç»“æœ
        draw_detection_results(frame, pose_results, hand_results)

        sequence.append(keypoints)

        if len(sequence) > seq_len:
            sequence.pop(0)

        if len(sequence) == seq_len:
            # è´¨é‡æ£€æŸ¥
            sequence_array = np.array(sequence)
            non_zero_ratio = np.mean(sequence_array != 0)

            if non_zero_ratio > adaptive_params['quality_threshold']:
                # æ ‡å‡†åŒ–å’Œé¢„æµ‹
                seq_reshaped = sequence_array.reshape(-1, sequence_array.shape[-1])
                seq_normalized = scaler.transform(seq_reshaped)
                seq_normalized = seq_normalized.reshape(sequence_array.shape)

                X = torch.tensor(seq_normalized, dtype=torch.float32).unsqueeze(0)

                try:
                    with torch.no_grad():
                        outputs = model(X)
                        probabilities = torch.softmax(outputs, dim=1)
                        confidence, predicted = torch.max(probabilities, 1)

                        conf_value = confidence.item()

                        if conf_value > adaptive_params['confidence_threshold']:
                            predictions.append(predicted.item())
                            confidences.append(conf_value)

                            if len(predictions) > adaptive_params['buffer_size']:
                                predictions.pop(0)
                                confidences.pop(0)

                        # æ˜¾ç¤ºç»“æœ
                        display_results(frame, predictions, confidences, GESTURES, adaptive_params)

                except Exception as e:
                    print(f"é¢„æµ‹é”™è¯¯: {e}")
                    cv2.putText(frame, f'Error: {str(e)[:30]}', (10, 30),
                              cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            else:
                cv2.putText(frame, f'Low Quality: {non_zero_ratio:.2f}',
                           (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

        cv2.imshow('Adaptive Gesture Recognition', frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
    extractor.close()

def analyze_environment(cap, extractor, sample_frames=100):
    """åˆ†æç¯å¢ƒæ¡ä»¶"""
    print(f"   é‡‡æ · {sample_frames} å¸§è¿›è¡Œç¯å¢ƒåˆ†æ...")

    brightness_values = []
    contrast_values = []
    detection_rates = []

    for i in range(sample_frames):
        ret, frame = cap.read()
        if not ret:
            break

        # åˆ†æå›¾åƒè´¨é‡
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        brightness_values.append(np.mean(gray))
        contrast_values.append(np.std(gray))

        # åˆ†ææ£€æµ‹è´¨é‡
        try:
            keypoints, _, _ = extractor.extract_features(frame)
            detection_rate = np.mean(keypoints != 0)
            detection_rates.append(detection_rate)
        except:
            detection_rates.append(0)

    stats = {
        'avg_brightness': np.mean(brightness_values),
        'avg_contrast': np.mean(contrast_values),
        'avg_detection_rate': np.mean(detection_rates),
        'brightness_std': np.std(brightness_values),
        'detection_stability': 1 - np.std(detection_rates)  # ç¨³å®šæ€§æŒ‡æ ‡
    }

    print("\n   ç¯å¢ƒç»Ÿè®¡:")
    print(f"     å¹³å‡äº®åº¦: {stats['avg_brightness']:.1f}")
    print(f"     å¹³å‡å¯¹æ¯”åº¦: {stats['avg_contrast']:.1f}")
    print(f"     æ£€æµ‹ç‡: {stats['avg_detection_rate']:.1f}")
    print(f"     äº®åº¦ç¨³å®šæ€§: {stats['brightness_std']:.1f}")
    print(f"     æ£€æµ‹ç¨³å®šæ€§: {stats['detection_stability']:.1f}")
    return stats

def get_adaptive_parameters(env_stats):
    """æ ¹æ®ç¯å¢ƒç»Ÿè®¡è°ƒæ•´å‚æ•°"""

    # åŸºç¡€å‚æ•°
    params = {
        'confidence_threshold': 0.7,
        'buffer_size': 8,
        'quality_threshold': 0.3
    }

    # æ ¹æ®æ£€æµ‹è´¨é‡è°ƒæ•´
    detection_rate = env_stats['avg_detection_rate']
    if detection_rate > 0.7:
        params['confidence_threshold'] = 0.6  # æ£€æµ‹å¥½æ—¶é™ä½é˜ˆå€¼
        params['quality_threshold'] = 0.2
    elif detection_rate > 0.5:
        params['confidence_threshold'] = 0.75  # æ£€æµ‹ä¸€èˆ¬æ—¶æé«˜é˜ˆå€¼
        params['quality_threshold'] = 0.35
    else:
        params['confidence_threshold'] = 0.8  # æ£€æµ‹å·®æ—¶æé«˜é˜ˆå€¼
        params['quality_threshold'] = 0.4

    # æ ¹æ®äº®åº¦è°ƒæ•´
    brightness = env_stats['avg_brightness']
    if brightness < 80:
        params['buffer_size'] = 12  # æš—ç¯å¢ƒä½¿ç”¨æ›´å¤§ç¼“å†²åŒº
    elif brightness > 200:
        params['buffer_size'] = 6   # äº®ç¯å¢ƒä½¿ç”¨å°ç¼“å†²åŒº

    return params

def draw_detection_results(frame, pose_results, hand_results):
    """ç»˜åˆ¶æ£€æµ‹ç»“æœ"""
    try:
        import mediapipe as mp

        # æ ¹æ®APIç‰ˆæœ¬ç»˜åˆ¶
        if hasattr(mp, 'solutions'):
            # æ—§ç‰ˆAPI
            mp_drawing = mp.solutions.drawing_utils
            mp_hands = mp.solutions.hands
            mp_pose = mp.solutions.pose

            if pose_results and pose_results.pose_landmarks:
                mp_drawing.draw_landmarks(frame, pose_results.pose_landmarks, mp_pose.POSE_CONNECTIONS)

            if hand_results and hand_results.multi_hand_landmarks:
                for hand_landmarks in hand_results.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
        else:
            # æ–°ç‰ˆAPI - ç®€åŒ–ç»˜åˆ¶
            cv2.putText(frame, 'Detection active', (10, frame.shape[0] - 20),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

    except Exception as e:
        cv2.putText(frame, f'Draw error: {str(e)[:20]}', (10, frame.shape[0] - 20),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)

def display_results(frame, predictions, confidences, GESTURES, params):
    """æ˜¾ç¤ºé¢„æµ‹ç»“æœ"""
    if len(predictions) >= 3:
        most_common = Counter(predictions).most_common(1)[0][0]
        avg_confidence = np.mean(confidences[-len(predictions):])
        gesture = GESTURES[most_common]

        # æ ¹æ®ç½®ä¿¡åº¦é€‰æ‹©é¢œè‰²
        if avg_confidence > 0.8:
            color = (0, 255, 0)  # ç»¿è‰² - é«˜ç½®ä¿¡åº¦
        elif avg_confidence > 0.6:
            color = (0, 255, 255)  # é»„è‰² - ä¸­ç­‰ç½®ä¿¡åº¦
        else:
            color = (0, 165, 255)  # æ©™è‰² - ä½ç½®ä¿¡åº¦

        cv2.putText(frame, f'Gesture: {gesture}', (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)
        cv2.putText(frame, f'Confidence: {avg_confidence:.2f}', (10, 70),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
    elif len(predictions) > 0:
        gesture = GESTURES[predictions[-1]]
        cv2.putText(frame, f'Detecting: {gesture}', (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
    else:
        cv2.putText(frame, 'Analyzing...', (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

    # æ˜¾ç¤ºç¯å¢ƒå‚æ•°
    cv2.putText(frame, f'Threshold: {params["confidence_threshold"]:.2f}', (10, frame.shape[0] - 60),
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
    cv2.putText(frame, f'Buffer: {params["buffer_size"]}', (10, frame.shape[0] - 40),
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)
    cv2.putText(frame, f'Quality: {params["quality_threshold"]:.2f}', (10, frame.shape[0] - 20),
               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 1)

if __name__ == "__main__":
    adaptive_inference()